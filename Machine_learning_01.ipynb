{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db46bc8",
   "metadata": {},
   "source": [
    "### Machine Learning Assignment-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5799e",
   "metadata": {},
   "source": [
    "Submitted by: G.Murali Divya Teja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e4398",
   "metadata": {},
   "source": [
    "1. What is Linear Regression?\n",
    "2. How we can calculate error in the Linear Regression?\n",
    "3. What is Error in Regression?\n",
    "4. Difference between Loss function and Cost function?\n",
    "5. What is the difference between MSE,MAE and RMSE?\n",
    "6. Explain about the intercept term means?\n",
    "7. Write all assumptions for linear regression?\n",
    "8. How Hypothesis testing used in the Linear Regression?\n",
    "9. How would you decide importance of variable for the multivariate regression?\n",
    "10. Differenece between R^2 and Adjusted-R^2 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39daaf5e",
   "metadata": {},
   "source": [
    "#### 1. What is Linear Regerssion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7290d74",
   "metadata": {},
   "source": [
    "__Ans:__ Linear regression is a quiet and the simplest statistical regression method used for predictive analysis in machine learning. Linear regression shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent(output) variable i.e. Y-axis, called linear regression. Linear Regression is a machine learning algorithm based on supervised learning.\n",
    "It is mostly used for finding out the relationship between variables and forecasting.\n",
    "\n",
    " Mathematical Equation for Linear Regerssion     $\\hat{Y} = \\hat{\\beta}_{0} + \\sum \\limits _{j=1} ^{p} X_{j}\\hat{\\beta}_{j} $\n",
    " \n",
    " here  $\\hat{Y}$ is the dependent variable, X is the independent variable, $\\hat{\\beta}_{0}$ is the intercepter, $\\hat{\\beta}_{j}$ is the slope of the equation\n",
    " \n",
    "Here in linear regresion we fiding the best fit line for the dependent variable to get best fitline $\\hat{\\beta}_{0}$ ,$\\hat{\\beta}_{j}$  interceptor and slope will be modified using Gradient Decent. \n",
    "\n",
    "Types of Linear Regression\n",
    "1. Simple Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "3. Polynomial Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c5283",
   "metadata": {},
   "source": [
    "#### 2.How we can calculate error in the Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed321a",
   "metadata": {},
   "source": [
    "__Ans:__ In Linear Regeression, after predicting the best fit line of dependent variable for the given dataset of independent variable, we compare the predicted values with Actual values, if we have any difference between actual and predicted values we call it as error or residual.\n",
    "\n",
    "        Error(or) residual  = predicted_value - Actual_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616812a",
   "metadata": {},
   "source": [
    "#### 3. What is Error in Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46021fac",
   "metadata": {},
   "source": [
    "__Ans:__ In regression, the difference between the observed value of the dependent variable(yi) and the predicted value(predicted) is called the residuals.\n",
    "            \n",
    "            Error(or) residual  = predicted_value - Actual_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6c5ee0",
   "metadata": {},
   "source": [
    "#### 4. What is Difference between Loss function and Cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10334847",
   "metadata": {},
   "source": [
    "__Ans:__ \n",
    "In other words, the loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733c7d9",
   "metadata": {},
   "source": [
    "#### 5.What is the difference between MSE,MAE and RMSE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a9350",
   "metadata": {},
   "source": [
    "__Ans__ In Machine Learning Linear Regression we have 3 different loss fucntions \n",
    "MSE: Mean Square Error: The Mean Squared Error (MSE) is the simplest and most common loss function, often taught in  Machine Learning. To calculate the MSE, you take the difference between your model’s predictions and the actual value, square it, and average it out across the whole dataset.\n",
    "\n",
    "MSE will never be negative, since we are always squaring the errors. The MSE is formally defined by the following equation:\n",
    "\n",
    "               MSE = (1/n)* sum of  (Ypred- Yactual)**2    here n is total number of the datapoints\n",
    "               \n",
    "MAE: The Mean Absolute Error (MAE) is only slightly different in definition from the MSE, but interestingly provides almost exactly opposite properties! To calculate the MAE, you take the difference between your model’s predictions and the ground truth, apply the absolute value to that difference, and then average it out across the whole dataset.\n",
    "\n",
    "The MAE, like the MSE, will never be negative since in this case we are always taking the absolute value of the errors. The MAE is formally defined by the following equation:\n",
    "\n",
    "                  MAE = (1/n)*|Ypred-Yactual|\n",
    "                  \n",
    "RMSE: The most common metric for evaluating linear regression model performance is called root mean squared error, or RMSE. The basic idea is to measure how bad/erroneous the model’s predictions are when compared to actual observed values. So a high RMSE is “bad” and a low RMSE is “good”.\n",
    "\n",
    "the difference between the observed and predicted values is called the residual. The mean squared error (MSE) is the average of all the squared residuals. Then the RMSE just takes the square root of that, which puts the metric back in the response variable scale.\n",
    "    \n",
    "                 RMSE = squre root of((1/n)* sum of  (Ypred- Yactual)**2) = sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ba64d",
   "metadata": {},
   "source": [
    "#### 6.Explain about the intercept term means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0950d",
   "metadata": {},
   "source": [
    "In Linear Regression, we predict the values of y using the slope and intercept using y=mx+c here m is slope, and c is intercept.\n",
    "An intercept is a point on the y-axis, through which the slope of the line passes. It is the y-coordinate of a point where a straight line or a curve intersects the y-axis.\n",
    "\n",
    "if  x=0 then y = c and c!=0, c is a constant value her c is constant value with intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43124f27",
   "metadata": {},
   "source": [
    "#### 7.Write all assumptions for linear regression?"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "84fea4fd",
   "metadata": {},
   "source": [
    "__Ans:__ Linear regression is the core process for various prediction analytics. By definition, linear regression refers to fitting of two continuous variables of interest. Not all datasets can be fitted into a linear fashion. There are few assumptions that must be fulfilled before jumping into the regression analysis. Some of those are very critical for model’s evaluation.\n",
    "\n",
    "1. Linearity\n",
    "2. Multicollinearity\n",
    "3. Homoscedasticity\n",
    "4. Multivariate normality\n",
    "5. Autocorrelation\n",
    "\n",
    "\n",
    "__Linearity:__ There variable that we are trying to fit should maintain a linear relationship. If there is no linear relationship, the data can be transformed to make it linear. \n",
    "Check scatterplot of the response variable against all the independent variables.\n",
    "If no linearity is observed, transform the data.\n",
    "\n",
    "__Multicollinearity__ Multicollinearity is observed when two or more independent variables are correlated to one another. If that is the case, the model’s estimation of the coefficients will be systematically wrong. One can check Variance Inflation Factor (VIF) to determine the variables which are highly correlated and potentially drop those variables from the model. R² is a measure of how correlated the variables are and VIF is determined from this R² value. If the variables have high correlation, VIF value shoots up. Typically VIF value >5 indicates the presence of multicollinearity. \n",
    "\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "__Homoscedasticity:__ It requires equal variance among the data points on both side of the linear fit. If it is not the case, the data is heteroscedastic. Typically the quality of the data gives rise to this heteroscedastic behavior. \n",
    "\n",
    "__Multivariate normality__ This assumption states that the residuals from the model is normally distributed. After determining the model parameters, it is good to check the distribution of the residuals. Apart from the visual of the distribution, one should check the Q-Q plot for better understanding of the distribution. Readers are encouraged to go through the basics and implementation of Q-Q\n",
    "\n",
    "__Autocorrelation__\n",
    "Apart from these scenarios, there is “no autocorrelation” assumption which basically tells us that there should be no specific pattern in the residual scatterplot. One residual at a specific location should not be dependent on it’s surrounding residuals. Constant variance assumption is somewhat related to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02f81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
